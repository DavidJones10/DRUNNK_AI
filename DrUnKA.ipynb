{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYaTbj5gPydC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from IPython.display import Audio\n",
        "import librosa\n",
        "import json\n",
        "from IPython.core.display import display\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5lMdwGUvD0-h",
        "outputId": "c687091d-3dd1-4375-f582-b3432d4190af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.13.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHBgdbjtP07v",
        "outputId": "fd4884f7-8e4a-4d75-9fab-f97c2526f01e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyperparameters\n",
        "#@title Hyperparameters\n",
        "learning_rate = 0.0005 #param {type:\"raw\"}\n",
        "num_epochs_to_train =  120#param {type:\"integer\"}\n",
        "batch_size =  64#param {type:\"integer\"}\n",
        "vector_dimension = 64 #param {type:\"integer\"}\n",
        "\n",
        "hop=256               #hop size (window size = 4*hop)\n",
        "frame_size=512\n",
        "fs=16000              #sampling rate\n",
        "min_level_db=-100     #reference values to normalize data\n",
        "ref_level_db=20\n",
        "\n",
        "LEARNING_RATE = learning_rate\n",
        "BATCH_SIZE = batch_size\n",
        "EPOCHS = num_epochs_to_train\n",
        "VECTOR_DIM=vector_dimension\n",
        "\n",
        "shape=128           #length of time axis of split specrograms\n"
      ],
      "metadata": {
        "id": "wkkPQn86ZRtD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare Dataset\n",
        "def prepareDataset(data):\n",
        "  numberOfClips = 1000 #param #number of audio files to use in training\n",
        "  smallerData = data.take(numberOfClips)\n",
        "  # get audio arrays\n",
        "  specArrays = []\n",
        "  for i in smallerData:\n",
        "    audio = i['audio'].numpy()\n",
        "    stft = librosa.stft(audio, n_fft=frame_size, hop_length=hop)[:-1]\n",
        "    spectrogram = np.abs(stft)\n",
        "    log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
        "    norm_array = (log_spectrogram - log_spectrogram.min()) / (log_spectrogram.max() - log_spectrogram.min())\n",
        "     # create normalized spectrogram arrays\n",
        "    specArrays.append(norm_array)\n",
        "  specArrays = np.array(specArrays,dtype=np.float32)\n",
        "  return specArrays\n",
        "\n",
        "x_train = prepareDataset(data)\n",
        "x_train = x_train[..., np.newaxis]"
      ],
      "metadata": {
        "id": "bLMLIi8JP8nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load NSynth Dataset\n",
        "\n",
        "data = tfds.load(\"nsynth\", split='train[50%:]', shuffle_files=True, data_dir=\"data\")"
      ],
      "metadata": {
        "id": "lyHWcoAYP3Ht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c31cd703-348f-48a1-c962-73fdcadf9c8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You use TensorFlow DType <dtype: 'float32'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to float32.\n",
            "WARNING:absl:You use TensorFlow DType <dtype: 'string'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to object.\n",
            "WARNING:absl:You use TensorFlow DType <dtype: 'bool'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to bool.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Variational Autoencoder Class\n",
        "from keras import Model\n",
        "from keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\n",
        "from keras.optimizers.legacy import Adam\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras import backend as K\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "class VAE:\n",
        "  def __init__(self,\n",
        "             input_shape,\n",
        "             conv_filters,\n",
        "             conv_kernels,\n",
        "             conv_strides,\n",
        "             latent_space_dim):\n",
        "    self.input_shape = input_shape\n",
        "    self.conv_filters = conv_filters\n",
        "    self.conv_kernels = conv_kernels\n",
        "    self.conv_strides = conv_strides\n",
        "    self.latent_space_dim = latent_space_dim\n",
        "    self.reconstruction_loss_weight = 1000000\n",
        "\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    self.model = None\n",
        "\n",
        "    self.num_conv_layers = len(conv_filters)-1\n",
        "    self.shape_before_bottleneck = None\n",
        "    self.model_input = None\n",
        "\n",
        "    self.build()\n",
        "\n",
        "  def summary(self):\n",
        "    self.encoder.summary()\n",
        "    self.decoder.summary()\n",
        "    self.model.summary()\n",
        "\n",
        "  def compile(self, learning_rate = .0001):\n",
        "    optimizer = Adam(learning_rate = learning_rate)\n",
        "    self.model.compile(optimizer=optimizer,\n",
        "                              loss=self.calculate_combined_loss,\n",
        "                           metrics=[self.calculate_reconstruction_loss,\n",
        "                                    self.calculate_kl_loss])\n",
        "\n",
        "  def train(self, x_train, batch_size, num_epochs):\n",
        "      self.model.fit(x_train,\n",
        "                     x_train,\n",
        "                     batch_size = batch_size,\n",
        "                     epochs = num_epochs,\n",
        "                     shuffle = True)\n",
        "\n",
        "  def save(self, save_folder=\".\"):\n",
        "      self.create_folder_if_it_doesnt_exist(save_folder)\n",
        "      self.save_parameters(save_folder)\n",
        "      self.save_weights(save_folder)\n",
        "\n",
        "  def load_weights(self, weights_path):\n",
        "      self.model.load_weights(weights_path)\n",
        "\n",
        "  def reconstruct(self, images):\n",
        "      latent_representations = self.encoder.predict(images)\n",
        "      reconstructed_images = self.decoder.predict(latent_representations)\n",
        "      return reconstructed_images, latent_representations\n",
        "\n",
        "  @classmethod\n",
        "  def load(cls, save_folder=\".\"):\n",
        "      parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "      with open(parameters_path, \"rb\") as f:\n",
        "          parameters = pickle.load(f)\n",
        "      autoencoder = VAE(*parameters)\n",
        "      weights_path = os.path.join(save_folder, \"weights.h5\")\n",
        "      autoencoder.load_weights(weights_path)\n",
        "      return autoencoder\n",
        "\n",
        "  def calculate_combined_loss(self, y_target, y_predicted):\n",
        "      reconstruction_loss = self.calculate_reconstruction_loss(y_target, y_predicted)\n",
        "      kl_loss = self.calculate_kl_loss(y_target, y_predicted)\n",
        "      combined_loss = self.reconstruction_loss_weight * reconstruction_loss\\\n",
        "                                                       + kl_loss\n",
        "      return combined_loss\n",
        "\n",
        "  def calculate_reconstruction_loss(self, y_target, y_predicted):\n",
        "      error = y_target - y_predicted\n",
        "      reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "      return reconstruction_loss\n",
        "\n",
        "  def calculate_kl_loss(self, y_target, y_predicted):\n",
        "      kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
        "                             K.exp(self.log_variance), axis=1)\n",
        "      return kl_loss\n",
        "\n",
        "  def create_folder_if_it_doesnt_exist(self, folder):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "  def save_parameters(self, save_folder):\n",
        "    parameters = [\n",
        "          self.input_shape,\n",
        "          self.conv_filters,\n",
        "          self.conv_kernels,\n",
        "          self.conv_strides,\n",
        "          self.latent_space_dim\n",
        "      ]\n",
        "    save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(parameters, f)\n",
        "\n",
        "  def save_weights(self, save_folder):\n",
        "    save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "    self.model.save_weights(save_path)\n",
        "\n",
        "  def build(self):\n",
        "    self.build_encoder()\n",
        "    self.build_decoder()\n",
        "    self.build_autoencoder()\n",
        "\n",
        "  def build_autoencoder(self):\n",
        "    model_input = self.model_input\n",
        "    model_output = self.decoder(self.encoder(model_input))\n",
        "    self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
        "  # decoder stuff=======================================================\n",
        "  def build_decoder(self):\n",
        "    input = self.add_decoder_input()\n",
        "    dense = self.add_dense_layer(input)\n",
        "    reshape = self.add_reshape_layer(dense)\n",
        "    transposed = self.add_conv_transpose_layers(reshape)\n",
        "    decoder_output = self.add_decoder_output(transposed)\n",
        "    self.decoder = Model(input, decoder_output, name=\"decoder\")\n",
        "\n",
        "  def add_decoder_input(self):\n",
        "    return Input(shape=self.latent_space_dim, name='decoder_input')\n",
        "\n",
        "  def add_dense_layer(self, decoder_input):\n",
        "    num_neurons = np.prod(self.shape_before_bottleneck)\n",
        "    dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
        "    return dense_layer\n",
        "\n",
        "  def add_reshape_layer(self, dense_layer):\n",
        "    return Reshape(self.shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "  def add_conv_transpose_layers(self, x):\n",
        "    for layer_idx in reversed(range(1, self.num_conv_layers)):\n",
        "      x = self.add_conv_transpose_layer(layer_idx, x)\n",
        "    return x\n",
        "\n",
        "  def add_conv_transpose_layer(self, layer_index, x):\n",
        "    layer_num = self.num_conv_layers - layer_index\n",
        "    conv_transpose_layer = Conv2DTranspose(filters=self.conv_filters[layer_index],\n",
        "                                           kernel_size=self.conv_kernels[layer_index],\n",
        "                                           strides=self.conv_strides[layer_index],\n",
        "                                           padding=\"same\",\n",
        "                                           name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "                                             )\n",
        "    x = conv_transpose_layer(x)\n",
        "    x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
        "    x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
        "    return x\n",
        "\n",
        "  def add_decoder_output(self, x):\n",
        "    conv_transpose_layer = Conv2DTranspose(filters=1,\n",
        "                                           kernel_size=self.conv_kernels[0],\n",
        "                                           strides=self.conv_strides[0],\n",
        "                                           padding=\"same\",\n",
        "                                           name=f\"decoder_conv_transpose_layer_{self.num_conv_layers}\"\n",
        "                                          )\n",
        "    x = conv_transpose_layer(x)\n",
        "    output_layer = Activation(\"sigmoid\", name=\"sigmoid_layer\")(x)\n",
        "    return output_layer\n",
        "\n",
        "  # encoder stuff=======================================================\n",
        "\n",
        "  def build_encoder(self):\n",
        "    encoder_input = self.add_encoder_input()\n",
        "    convLayers = self.add_conv_layers(encoder_input)\n",
        "    bottleneck = self.add_bottleneck(convLayers)\n",
        "    self.model_input = encoder_input\n",
        "    self.encoder  = Model(encoder_input, bottleneck, name='encoder')\n",
        "\n",
        "  def add_encoder_input(self):\n",
        "    return Input(shape=self.input_shape, name='encoder_input')\n",
        "\n",
        "  def add_conv_layers(self, input):\n",
        "    x = input\n",
        "    for layer_idx in range(self.num_conv_layers):\n",
        "      x = self.add_conv_layer(layer_idx, x)\n",
        "    return x\n",
        "\n",
        "  def add_conv_layer(self, idx, x):\n",
        "    layerNum = idx + 1\n",
        "    conv_layer = Conv2D(filters=self.conv_filters[idx],\n",
        "                        kernel_size=self.conv_kernels[idx],\n",
        "                        strides=self.conv_strides[idx],\n",
        "                        padding='same',\n",
        "                        name=f\"encoder_transpose_layer_{layerNum}\"\n",
        "                        )\n",
        "    x = conv_layer(x)\n",
        "    x = ReLU(name=f\"encoder_relu_{layerNum}\")(x)\n",
        "    x = BatchNormalization(name=f\"encoder_bn_{layerNum}\")(x)\n",
        "    return x\n",
        "\n",
        "  def add_bottleneck(self, x):\n",
        "    self.shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "    self.mu = Dense(self.latent_space_dim, name=\"mu\")(x)\n",
        "    self.log_variance = Dense(self.latent_space_dim,\n",
        "                                name=\"log_variance\")(x)\n",
        "    def sample_point_from_normal_distribution(args):\n",
        "        mu, log_variance = args\n",
        "        epsilon = K.random_normal(shape=K.shape(self.mu), mean=0.,\n",
        "                                    stddev=1.)\n",
        "        sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "        return sampled_point\n",
        "    x = Lambda(sample_point_from_normal_distribution,\n",
        "                 name=\"encoder_output\")([self.mu, self.log_variance])\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "ODlpo9dwTf5T"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg9eFlqGmwzf",
        "outputId": "a8be5b50-3cf3-44f6-f10e-f45e53dd4dea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 256, 251, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train the Model on the dataset\n",
        "def train(train_data, learning_rate, batch_size, epochs):\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  autoEncoder = VAE(input_shape=(256, 251, 1),\n",
        "                            conv_filters=(512, 256, 128, 64, 32),\n",
        "                            conv_kernels=(3, 3, 3, 3),\n",
        "                            conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "                            latent_space_dim=VECTOR_DIM\n",
        "                            )\n",
        "  autoEncoder.summary()\n",
        "  autoEncoder.compile(learning_rate)\n",
        "  autoEncoder.train(train_data, batch_size, epochs)\n",
        "  return autoEncoder\n",
        "\n",
        "autoencoder = train(train_data=x_train,\n",
        "                    learning_rate=LEARNING_RATE,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "U-_GK9wI_dC4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06a7418b-717e-4e51-9b81-7bfd9a22a5fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)  [(None, 256, 251, 1)]        0         []                            \n",
            "                                                                                                  \n",
            " encoder_transpose_layer_1   (None, 128, 126, 512)        5120      ['encoder_input[0][0]']       \n",
            " (Conv2D)                                                                                         \n",
            "                                                                                                  \n",
            " encoder_relu_1 (ReLU)       (None, 128, 126, 512)        0         ['encoder_transpose_layer_1[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " encoder_bn_1 (BatchNormali  (None, 128, 126, 512)        2048      ['encoder_relu_1[0][0]']      \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " encoder_transpose_layer_2   (None, 64, 63, 256)          1179904   ['encoder_bn_1[0][0]']        \n",
            " (Conv2D)                                                                                         \n",
            "                                                                                                  \n",
            " encoder_relu_2 (ReLU)       (None, 64, 63, 256)          0         ['encoder_transpose_layer_2[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " encoder_bn_2 (BatchNormali  (None, 64, 63, 256)          1024      ['encoder_relu_2[0][0]']      \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " encoder_transpose_layer_3   (None, 32, 32, 128)          295040    ['encoder_bn_2[0][0]']        \n",
            " (Conv2D)                                                                                         \n",
            "                                                                                                  \n",
            " encoder_relu_3 (ReLU)       (None, 32, 32, 128)          0         ['encoder_transpose_layer_3[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " encoder_bn_3 (BatchNormali  (None, 32, 32, 128)          512       ['encoder_relu_3[0][0]']      \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " encoder_transpose_layer_4   (None, 16, 16, 64)           73792     ['encoder_bn_3[0][0]']        \n",
            " (Conv2D)                                                                                         \n",
            "                                                                                                  \n",
            " encoder_relu_4 (ReLU)       (None, 16, 16, 64)           0         ['encoder_transpose_layer_4[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " encoder_bn_4 (BatchNormali  (None, 16, 16, 64)           256       ['encoder_relu_4[0][0]']      \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)         (None, 16384)                0         ['encoder_bn_4[0][0]']        \n",
            "                                                                                                  \n",
            " mu (Dense)                  (None, 64)                   1048640   ['flatten_4[0][0]']           \n",
            "                                                                                                  \n",
            " log_variance (Dense)        (None, 64)                   1048640   ['flatten_4[0][0]']           \n",
            "                                                                                                  \n",
            " encoder_output (Lambda)     (None, 64)                   0         ['mu[0][0]',                  \n",
            "                                                                     'log_variance[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3654976 (13.94 MB)\n",
            "Trainable params: 3653056 (13.94 MB)\n",
            "Non-trainable params: 1920 (7.50 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_input (InputLayer)  [(None, 64)]              0         \n",
            "                                                                 \n",
            " decoder_dense (Dense)       (None, 16384)             1064960   \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " decoder_conv_transpose_lay  (None, 32, 32, 64)        36928     \n",
            " er_1 (Conv2DTranspose)                                          \n",
            "                                                                 \n",
            " decoder_relu_1 (ReLU)       (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " decoder_bn_1 (BatchNormali  (None, 32, 32, 64)        256       \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " decoder_conv_transpose_lay  (None, 64, 64, 128)       73856     \n",
            " er_2 (Conv2DTranspose)                                          \n",
            "                                                                 \n",
            " decoder_relu_2 (ReLU)       (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " decoder_bn_2 (BatchNormali  (None, 64, 64, 128)       512       \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " decoder_conv_transpose_lay  (None, 128, 128, 256)     295168    \n",
            " er_3 (Conv2DTranspose)                                          \n",
            "                                                                 \n",
            " decoder_relu_3 (ReLU)       (None, 128, 128, 256)     0         \n",
            "                                                                 \n",
            " decoder_bn_3 (BatchNormali  (None, 128, 128, 256)     1024      \n",
            " zation)                                                         \n",
            "                                                                 \n",
            " decoder_conv_transpose_lay  (None, 256, 256, 1)       2305      \n",
            " er_4 (Conv2DTranspose)                                          \n",
            "                                                                 \n",
            " sigmoid_layer (Activation)  (None, 256, 256, 1)       0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1475009 (5.63 MB)\n",
            "Trainable params: 1474113 (5.62 MB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "_________________________________________________________________\n",
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_input (InputLayer)  [(None, 256, 251, 1)]     0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 64)                3654976   \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 256, 256, 1)       1475009   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5129985 (19.57 MB)\n",
            "Trainable params: 5127169 (19.56 MB)\n",
            "Non-trainable params: 2816 (11.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0dcadf950066>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mautoEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m autoencoder = train(train_data=x_train,\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-0dcadf950066>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, learning_rate, batch_size, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m                             )\n\u001b[1;32m     10\u001b[0m   \u001b[0mautoEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mautoEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mautoEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mautoEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-88480b71bf7c>\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     self.model.compile(optimizer=optimizer,\n\u001b[0m\u001b[1;32m     42\u001b[0m                               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_combined_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                            metrics=[self.calculate_reconstruction_loss,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;31m# Save all metric attributes per output of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_output_metric_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;31m# Set metric attributes on model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py\u001b[0m in \u001b[0;36m_cache_output_metric_attributes\u001b[0;34m(self, metrics, weighted_metrics)\u001b[0m\n\u001b[1;32m   2014\u001b[0m                 \u001b[0moutput_shapes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m         self._per_output_metrics = (\n\u001b[0;32m-> 2016\u001b[0;31m             training_utils_v1.collect_per_output_metric_info(\n\u001b[0m\u001b[1;32m   2017\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_utils_v1.py\u001b[0m in \u001b[0;36mcollect_per_output_metric_info\u001b[0;34m(metrics, output_names, output_shapes, loss_fns, from_serialized, is_weighted)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             )\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0mmetric_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_serialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_serialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0;31m# If the metric function is not stateful, we create a stateful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'method' object has no attribute '_from_serialized'"
          ]
        }
      ]
    }
  ]
}